# Multi-Threaded Web Page Downloader (C++)

A high-performance C++ command-line application designed to concurrently download web pages from a list of URLs. This project showcases advanced C++ concurrency patterns, robust resource management using RAII, and efficient network I/O with `libcurl`, making it ideal for demonstrating skills in system programming and distributed concepts.

## Features

* **Concurrent Downloads:** Utilizes a custom-built **Thread Pool** to perform multiple web page downloads in parallel, maximizing efficiency.
* **Robust Concurrency Model:** Implements a **Producer-Consumer pattern** using `std::mutex` and `std::condition_variable` to ensure efficient task distribution, prevent busy-waiting, and manage graceful thread shutdown.
* **Resource Acquisition Is Initialization (RAII):** Employs custom RAII wrappers (`CurlHandle`, `FileHandle`) to guarantee that `libcurl` handles and file pointers are properly cleaned up, preventing resource leaks even in the presence of errors.
* **Thread-Safe Logging:** Features a custom, thread-safe `Logger` utility that centralizes output to both the console and a dedicated log file, simplifying debugging and monitoring.
* **Automatic Retries with Backoff:** Implements a retry mechanism with exponential backoff for failed downloads, improving reliability against transient network issues.
* **Basic URL Validation:** Filters out invalid URL formats from the input list.
* **Atomic Progress Tracking:** Uses `std::atomic` for thread-safe tracking of completed downloads, providing accurate real-time progress updates.

## Project Structure

* `main.cpp`: Contains the complete source code for the multi-threaded downloader, including `ThreadPool`, RAII wrappers, `Logger`, and the main application logic.
* `urls.txt`: A plain text file where you list the URLs (one per line) that you want the downloader to fetch.
* `errors_and_logs.log`: The output log file generated by the `Logger` class, containing detailed program messages and error reports.
* `pageX.html`: Downloaded web pages will be saved as `page1.html`, `page2.html`, etc.

## Technologies Used

* **C++17 (or newer):** Leverages modern C++ features for robust and efficient programming.
* **`libcurl`:** A powerful and widely used client-side URL transfer library for making HTTP requests.
* **Standard C++ Concurrency Library:** `std::thread`, `std::mutex`, `std::condition_variable`, `std::atomic`, `std::function`.
* **Standard C++ Libraries:** `iostream`, `fstream`, `vector`, `string`, `queue`, `regex`, `chrono`, `iomanip`.

## Prerequisites

* **C++17 Compatible Compiler:** GCC, Clang, or MSVC.
* **`libcurl` Development Libraries:** You need `libcurl` installed on your system.

    * **On Debian/Ubuntu:**
        ```bash
        sudo apt-get update
        sudo apt-get install libcurl4-openssl-dev
        ```
    * **On Fedora/RHEL/CentOS:**
        ```bash
        sudo yum install libcurl-devel
        # OR (for newer Fedora/RHEL)
        sudo dnf install libcurl-devel
        ```
    * **On macOS (using Homebrew):**
        ```bash
        brew install curl
        ```

## How to Build and Run

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/sagili-manoj/MutliThread-WebPage-Downloader.git](https://github.com/sagili-manoj/MutliThread-WebPage-Downloader.git)
    cd MutliThread-WebPage-Downloader
    ```
2.  **Create `urls.txt`:** Create a file named `urls.txt` in the same directory as `main.cpp`. Populate it with the URLs you wish to download, one URL per line.
    ```
    [https://example.com](https://example.com)
    [https://www.google.com](https://www.google.com)
    [https://www.bing.com](https://www.bing.com)
    # Add more URLs here
    ```
3.  **Compile the code:**
    Use a C++17 compatible compiler and link against `libcurl`.
    ```bash
    g++ -std=c++17 -Wall -Wextra -pedantic main.cpp -lcurl -o multi_downloader
    ```
    * `-std=c++17`: Specifies the C++17 standard.
    * `-Wall -Wextra -pedantic`: Enables extensive warnings and strict adherence to the standard.
    * `main.cpp`: Your source file.
    * `-lcurl`: **Crucially**, links the `libcurl` library.
    * `-o multi_downloader`: Names the output executable `multi_downloader`.

4.  **Run the executable:**
    ```bash
    ./multi_downloader
    ```

## Learnings and Challenges

* **Designing a Robust Thread Pool:** The primary challenge was moving from basic `std::thread` usage to a full-fledged producer-consumer `ThreadPool`. This involved carefully implementing `std::mutex` and `std::condition_variable` to manage task queues, avoid busy-waiting, and ensure proper thread synchronization and graceful shutdown.
* **Advanced RAII for External Resources:** Crafting custom RAII wrappers (`CurlHandle` and `FileHandle`) was key to ensuring `libcurl` handles and file pointers were always released, even if network errors or other exceptions occurred during downloads. This significantly improved memory safety and resource hygiene.
* **Thread-Safe Logging:** Implementing a singleton `Logger` class with mutex protection was essential for managing concurrent output from multiple threads without interleaved or corrupted log messages.
* **Error Handling and Retries:** Developing an effective retry mechanism for network requests, including exponential backoff, taught valuable lessons about making network applications resilient to transient failures.
* **Understanding `libcurl`:** Gaining hands-on experience with `libcurl`'s API for making HTTP requests, setting options, and handling data callbacks.
* **Atomic Operations:** Utilizing `std::atomic` for counters, highlighting awareness of fine-grained thread synchronization for simple shared variables.
* 
## ðŸ“· Example Output
![Screenshot 2025-05-21 125927](https://github.com/user-attachments/assets/2402c649-1e1f-4380-b67c-3abe12e4557a)


Downloaded web pages will be saved in the same folder that the file is currently running on.


## Author

Sagili Manoj Kumar Reddy
